{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c663a7",
   "metadata": {},
   "source": [
    "## Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde1c71",
   "metadata": {},
   "source": [
    "### Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size = 0.5, random_state =4)\n",
    "# Use a very bad multiclass classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(max_depth =2)\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e46a9f",
   "metadata": {},
   "source": [
    "Measures that are commonly used in multilabel classification:\n",
    "Confusion matrix: tells about misclassification for each class. Ideally, in a perfect classification, all the cells that are not on the diagonal should be 0s.In the following example, you will instead see that class 0 (Setosa) is never misclassified, class 1 (Versicolor) is misclassified thrice as Virginica, and class 2 (Virginica) is misclassified twice as Versicolor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c79faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = plt.matshow(cm, cmap=plt.cm.autumn)\n",
    "plt.colorbar(img, fraction=0.045)\n",
    "for x in range(cm.shape[0]):\n",
    "    for y in range(cm.shape[1]):\n",
    "        plt.text(x, y, \"%0.2f\" % cm[x,y], \n",
    "            size=12, color='black', ha=\"center\", va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy: Accuracy is the portion of the predicted labels that are exactly equal to the real ones. \n",
    "#In other words, it's the percentage of overall correctly classified labels\n",
    "print ( metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Precision: It is a measure that is taken from the information retrieval world. It counts the number\n",
    "of relevant results in the result set. Equivalently, in a classification task, it counts the number of correct \n",
    "labels in each set of classified labels.Then, results are averaged on all of the labels.\n",
    "\n",
    "Recall: This is another concept taken from information retrieval. \n",
    "It counts the number of relevant results in the result set, compared to all of the relevant labels in the dataset. \n",
    "In classification tasks, this is the amount of correctly classified labels in the set divided by the total count\n",
    "of labels for that set. Finally, the results are averaged, just like in the following code\n",
    "\n",
    "F1 Score: This is the harmonic average of precision and recall, which is mostly used when dealing with \n",
    "unbalanced datasets in order to reveal if the classifier is performing well with all the classes:\"\"\"\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(Y_test, Y_pred, \n",
    "                            target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d515a34",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ac2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to predict real numbers or regression, many error measures are derived from Euclidean algebra\n",
    "#Mean absolute error or MAE: This is the mean L1 norm of the difference vector between the predicted and real values\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "mean_absolute_error([1.0, 0.0, 0.0], [0.0, 0.0, -1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fc3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean squared error or MSE: This is the mean L2 norm of the difference vector between the predicted and real values\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mean_squared_error([-10.0, 0.0, 0.0], [0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"R2 score: R2 is also known as the coefficient of determination. R2 determines how good a linear fit \n",
    "there is that exists between the predictors and the target variable.It takes values between 0 and 1 (inclusive); \n",
    "the higher R2 is, the better the model.  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ede18",
   "metadata": {},
   "source": [
    "## Testing and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A machine learning algorithm, by observing a series of examples and pairing them with their outcome,\n",
    "is able to extract a series of rules that can be successfully generalized to new examples by correctly guessing\n",
    "their resulting outcome. Such is the supervised learning approach, where it applies a series of highly specialized\n",
    "learning algorithms that we expect can correctly predict (and generalize) on any new data\"\"\"\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#64 numeric values from 0-16 of each 8*8 images\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18282615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using three different support vector machines for classification\n",
    "from sklearn import svm\n",
    "h1 = svm.LinearSVC(C=1.0)\n",
    "h2 = svm.SVC(kernel = \"rbf\", degree = 3, gamma = 0.001, C=1.0)\n",
    "h3 = svm.SVC(kernel=\"poly\", degree =3, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.fit(X,y)\n",
    "print (h1.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752193ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_random_state = 1\n",
    "X_train, X_test, y_train, y_test =train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=0.30, random_state=chosen_random_state)\n",
    "print (\"(X train shape %s, X test shape %s,y train shape %s, y test shape %s\" % (X_train.shape, X_test.shape, \n",
    "                            y_train.shape, y_test.shape))\n",
    "h1.fit(X_train,y_train)\n",
    "print (h1.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using validation set to compare the performance\n",
    "chosen_random_state = 1\n",
    "X_train, X_validation_test, y_train, y_validation_test = train_test_split(X, y, test_size=.40, \n",
    "                                                                          random_state=chosen_random_state)            \n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_validation_test, y_validation_test, \n",
    "                                                              test_size=.50, \n",
    "                                                              random_state=chosen_random_state)\n",
    "print (\"X train shape, %s, X validation shape %s, X test shape %s,/n y train shape %s, y validation shape %s, y test shape %s/n\" \n",
    "       % (X_train.shape, X_validation.shape, X_test.shape,  \n",
    "         y_train.shape, y_validation.shape, y_test.shape))\n",
    "for hypothesis in [h1, h2, h3]:\n",
    "        hypothesis.fit(X_train,y_train)\n",
    "        print (\"%s -> validation mean accuracy = %0.3f\" % (hypothesis,  \n",
    "        hypothesis.score(X_validation,y_validation))  )  \n",
    "h2.fit(X_train,y_train)\n",
    "print (\"n%s -> test mean accuracy = %0.3f\" % (h2,   \n",
    "h2.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e09156",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0115603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The idea is to divide the training data into a certain number of partitions (called folds) and train the model\n",
    "#as many times as the number of partitions there are\n",
    "# After every model training, it will test the result on the fold that is left out and store it away.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "choosen_random_state = 1\n",
    "cv_folds = 10 # Try 3, 5 or 20\n",
    "eval_scoring='accuracy' # Try also f1\n",
    "workers = -1 # this will use all your CPU power\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y, \n",
    "                                    test_size=0.30, \n",
    "                                    random_state=choosen_random_state)\n",
    "for hypothesis in [h1, h2, h3]:\n",
    "    scores = cross_val_score(hypothesis, \n",
    "                     X_train, y_train, \n",
    "                     cv=cv_folds, scoring= eval_scoring, n_jobs=workers)\n",
    "    print (\"%s -> cross validation accuracy: mean = %0.3f \\\n",
    "            std = %0.3f\" % (hypothesis, np.mean(scores), \n",
    "                            np.std(scores))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
