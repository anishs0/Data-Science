{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1795cf47",
   "metadata": {},
   "source": [
    "# Data Loading with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb628cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in local repo\n",
    "iris_filename = \"irisdata.csv\"\n",
    "iris = pd.read_csv(iris_filename)\n",
    "iris.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be26fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = pd.read_csv(iris_filename, sep=\",\", decimal=\".\", header =None,\n",
    "                   names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"target\"])\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457aefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading from the internet\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "set1 = urllib.request.Request(url)\n",
    "iris_p = urllib.request.urlopen(set1)\n",
    "iris_other = pd.read_csv(iris_p, sep=',', decimal='.',\n",
    "header=None, names= ['sepal_length', 'sepal_width',\n",
    "                         'petal_length', 'petal_width', \n",
    "                         'target'])\n",
    "iris_other.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns #To get the name of columns\n",
    "#The result looks like a list, but is actually a pandas index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris[\"target\"]\n",
    "y\n",
    "#Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405431ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[['sepal_length', 'sepal_width']]\n",
    "X\n",
    "#getting a list of columns referring to them by their indexes\n",
    "#Pandas DataFrame(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a64afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the dimensions of the dataset\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c34e7",
   "metadata": {},
   "source": [
    "### Dealing with problematic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset = pd.read_csv('a_loading_example_1.csv')\n",
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset = pd.read_csv('a_loading_example_1.csv', parse_dates=[0])\n",
    "fake_dataset\n",
    "#parsing date as date from integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7e754",
   "metadata": {},
   "source": [
    "Getting rid of missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160bd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing Nan with a more meaningful number\n",
    "fake_dataset.fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de86c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing with -ve constant to mark difference\n",
    "fake_dataset.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset.fillna(fake_dataset.mean(axis = 0))\n",
    "#axis = 0 spans the rows\n",
    "#axis = 1 spans the column's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ec8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dataset = pd.read_csv('a_loading_example_2.csv', error_bad_lines=False)\n",
    "bad_dataset\n",
    "#Ignoring the lines causing exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b68a3",
   "metadata": {},
   "source": [
    "### Dealing with big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk and load a file\n",
    "iris_chunks = pd.read_csv(iris_filename, header = None,\n",
    "                          names = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"], chunksize=10)\n",
    "for chunk in iris_chunks:\n",
    "    print(\"Shape:\", chunk.shape)\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff18fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifically asking for an iterator to decide the length\n",
    "iris_iterator = pd.read_csv(iris_filename, header=None,\n",
    "                                 names=['C1', 'C2', 'C3', 'C4', 'C5'], \n",
    "                                 iterator=True)  \n",
    "print(iris_iterator.get_chunk(10).shape)\n",
    "print(iris_iterator.get_chunk(20).shape)\n",
    "piece = iris_iterator.get_chunk(2)\n",
    "piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#reader\n",
    "#DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb59db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(iris_filename, 'rt') as data_stream:\n",
    "      # 'rt' mode\n",
    "    for n, row in enumerate(csv.DictReader(data_stream,\n",
    "           fieldnames = ['sepal_length', 'sepal_width',\n",
    "                         'petal_length', 'petal_width', \n",
    "                         'target'],\n",
    "           dialect='excel')):\n",
    "              if n== 0:\n",
    "                  print (n, row)\n",
    "              else:\n",
    "                  break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(iris_filename, 'rt') as data_stream:\n",
    "    for n, row in enumerate(csv.reader(data_stream,\n",
    "           \n",
    "           dialect='excel')):\n",
    "        if n== 0:\n",
    "            print (n, row)\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9034595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def batch_read(filename, batch=5):\n",
    "    with open (filename, \"rt\") as data_stream: #open data stream\n",
    "        batch_output = list() #reset the batch\n",
    "        #Iterate over the file\n",
    "        for n, row in enumerate(csv.reader(data_stream, dialect = \"excel\")):\n",
    "            #if the batch is of the right size\n",
    "            if n>0 and n % batch ==0:\n",
    "                #yield back the batch as an ndarray\n",
    "                yield(np.array(batch_output))\n",
    "                #reset the batch and restart\n",
    "                batch_output = list()\n",
    "                #otherwise add the row to the batch\n",
    "            batch_output.append(row)\n",
    "            #when the loop is over, yield what's left\n",
    "        yield(np.array(batch_output))\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ddd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input in batch_read(iris_filename, batch = 3):\n",
    "    print(batch_input)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab1e9d",
   "metadata": {},
   "source": [
    "### Accessing other format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using SQL and SQLite to store away some data and retrieve a filtered version of it. Queries to drop any previous \n",
    "data table of the same name, and to create a new table that's capable of keeping the date, city, temperature, \n",
    "and destination data\n",
    "\n",
    "\"\"\"\n",
    "import sqlite3\n",
    "drop_query = \"DROP TABLE IF EXISTS temp_data;\"\n",
    "create_query = \"CREATE TABLE temp_data \\\n",
    "                (date INTEGER, city VARCHAR(80), \\\n",
    "                temperature REAL, destination INTEGER);\"\n",
    "connection = sqlite3.connect(\"example.db\")\n",
    "connection.execute(drop_query)\n",
    "connection.execute(create_query)\n",
    "connection.commit()\n",
    "data = [(20140910, \"Rome\",   80.0, 0),\n",
    "        (20140910, \"Berlin\", 50.0, 0),\n",
    "        (20140910, \"Wien\",   32.0, 1),\n",
    "        (20140911, \"Paris\",  65.0, 0)]\n",
    "insert_query = \"INSERT INTO temp_data VALUES(?,?,?,?)\"\n",
    "connection.executemany(insert_query, data)\n",
    "connection.commit()\n",
    "selection_query = \"SELECT date, city, temperature, destination \\\n",
    "                    FROM temp_data WHERE date = 20140910\"\n",
    "retrieved = pd.read_sql_query(selection_query, connection)\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with HDF5 format\n",
    "storage = pd.HDFStore(\"example.h5\")\n",
    "storage['iris'] = iris\n",
    "storage.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = pd.HDFStore('example.h5')\n",
    "storage.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_iris_upload = storage['iris']\n",
    "type(fast_iris_upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2ea54",
   "metadata": {},
   "source": [
    "### Putting Data Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_own_dataset = pd.DataFrame({\"Col1\": range(5),\n",
    "                               \"Col2\": [1.0] * 5,\n",
    "                               \"Col3\": 1.0,\n",
    "                               \"Col4\": \"Hello World!\"})\n",
    "my_own_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col5 = pd.Series([4, 3, 2, 1, 0])\n",
    "col6 = pd.Series([0, 0, 1, 1, 1])\n",
    "a_new_dataset = pd.concat([col5, col6], axis=1,ignore_index = True, keys=['Col5', 'Col6'])\n",
    "\n",
    "my_new_dataset = pd.concat([my_own_dataset, a_new_dataset], axis=1)\n",
    "my_new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628d8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
